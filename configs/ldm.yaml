dataset:
  fname: 'facies_5000.npy'  #'dataset/facies'
  train_ratio: 0.8
  in_channels: 4
  n_categories: 4
  target_input_dim:
    - 128
    - 128
  batch_sizes:
    stage1: 64
    stage2: 16
  num_workers: 0

trainer_params:
  gpu_idx: 0
  LR:  # learning rate
    stage1: 0.001
    stage2: 0.0001
  max_epochs:
    stage1: 200
  max_num_steps:  # it uses steps instead of epochs due to the DDPM implementation
    stage2: 14000
  save_period_in_epoch: 10
  check_val_every_n_epoch: 1

# encoder:
#   dim: 64
#   bottleneck_dim: 4    # small codebook_dim makes the diffusion modeling easier.
#   n_resnet_blocks: 4
#   downsampling_rate: 2  # the actual compression rate is `downsampling_rate ** 2` given both height and width.
#   output_norm: True  # normalize `z` to be within [-1, 1]
encoder:
  init_dim: 128
  hid_dim: 4  # bottleneck dim
  downsample_rate: 2  # the actual compression rate is `downsampling_rate ** 2` given both height and width.
  n_resnet_blocks: 4
  bottleneck_dim: 4    # small codebook_dim makes the diffusion modeling easier.
  output_norm: True  # normalize `z` to be within [-1, 1]

# decoder:
#   n_resnet_blocks: 4

VQ-VAE:  # hyper-parameter choice is made based on the LDM paper
  codebook_size: 256

diffusion:
  stage1_ckpt_fname: 'stage1.ckpt'  # saved file name for the model from stage 1
  unet:
    dim: 64
    self_condition: False
  p_unconditional: 0.1  # rate for unconditional generation during training
  classifier_free_guidance_scale: 1.0
  preserv_loss_weight: 1
  save_and_sample_every: 1000  # {100, 1000}
  num_samples: 9

sampling:
  trained_stage2_module_fname: 'stage2_a-14.ckpt'  # within `methods/ldm/saved_models/`
